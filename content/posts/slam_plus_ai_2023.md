+++
title = '[ModuCon 2023] SLAM + AI 강연 정리'
date = 2023-06-12T11:02:27+09:00
draft = false
tags = ['robotics', 'conference']
+++
2023년 2월 유튜브 모두의연구소 채널에서 공간지능(Spatial AI)을 주제로 한 강연을 듣고 그 내용을 정리하였다.
<!--more-->

---

## SLAM 이란?

### Intro

Simultaneous Localization and Mapping: 동시적 위치 추정 및 지도 작성
- 되게 멋있는 이름이다. 원래는 슬램이라는 이름을 가져 더 인기를 끌기 전에는 다른 이름도 있었다는데, 그때까지는 지금보다도 더 인기가 없었다고 한다. 이름도 재미가 없으니까. 근데 갑자기 슬램 막 이러니까 사람들이 막 뭐야 뭐야 이러면서 그때부터 좀 뜨기 시작했다고 한다.

- 개인적으로 Simultaneous 하다는 게 가장 중요한 것 같다. 이 simultaneous라는 단어가 SLAM 기술을 한 번에 다 표현하는 방법이라고 생각한다. 반대로 얘기하면, 왜 simultaneous한 지 이해를 못했다면 슬램을 이해하지 못한 거나 마찬가지라고 생각한다.

### Localization

위치 추정이라는 것은, 어떤 공간이 있을 때, 예를 들어서 여기 오피스가 있을 때, 내가 이 공간에 대한 지도를 가지고 있다고 가정하면, 지도를 가지고 있는 상황에서 내가 여기서 눈을 감고 있다가 눈을 딱 떴다고 생각해보자. 

- 그러면 저기 벽이 있고 여기 여기 쪽에 창문이 있고 저쪽에 커피 머신이 있고 라는 게 보인다. 이 때, 내가 지도를 가지고 있으면, 아. 커피 머신이 여기쯤 있구나. 지도의 창문이 여기 있구나. 여기에 벽이 있구나. 그러면 나는 이쯤 있겠다. 를 알 수 있다.

- 위치 추정을 하는 방법은 사실 굉장히 많다. 지도는 다양한 형태의 지도가 있을 수 있고, 그 지도를 바탕으로 위치 추정을 하는 방법도 굉장히 다양하다. 카메라 기반으로도 할 수 있고 라이더 기반으로도 할 수 있다.
   1. 가장 쉬운 방법은 GPS
   - 하늘에 있는 위성들이 삼각 측량을 해서 지구 위에 내가 어디 있는지를 알려줌
   2. 라이더 기반 Localization
   3. 카메라 기반 Visual localization

### Mapping

지도 작성 기술은 반대이다. Localization은 지도가 있을 때 나의 위치를 찾기인데, mapping 기술은 거꾸로 내 위치를 알고 있을 때 지도를 만들기이다. 근데 단순히 내가 그냥 여기 한 시점에 있으면서 주변을 딱 둘러봤을 때 물론 지도를 만들 수는 있겠지만, 내가 안 보이는 부분에 대해서는 어떻게 생겼는지 모르니까 지도를 못 만든다. 그럼 내가 이렇게 좀 이동을 해서 보면, 이제 보인다.

- 근데 이렇게 이동해서 지도를 정확하게 만들려면, 내가 정확히 이전 위치에서 여기까지 얼마나 움직였는지를 알아야 된다. 그게 아니라면 제가 저기서 본 거랑 여기서 본 거랑 아무런 관계도 없기 때문에 이걸로 뭐 무슨 지도를 어떻게 만들라는 거지? 이렇게 될 수가 있다. 
- 그냥 비슷하게 생긴 공간이 이어붙어져 있는 거 아닌가 할 수도 있는데, 그게 아니라 실제로 하나의 공간인데 다른 시점에서 본 겹치는 영역이 있고 안 겹치는 영역을 덧대어서 그리면 지도가 된다. 이렇게 볼 수 있다.

**지도를 그리는 방식이 사실 또 굉장히 많다**
   - 왼쪽 위에 보이는 건 테슬라에서 최근에 발표한 occupancy network.
   - 오른쪽 위에 보이는 것처럼 메쉬 형태로도 할 수 있음. 삼각형으로 덧대어서 붙여가지고 벽이 있는 곳과 없는 곳을 구분하기
   - 왼쪽 밑에는 포인트 클라우드
   - 오른쪽 밑이 조금 생소할 수도 있는데 Semantic map. 최근 많이 유명해지고 있는 지도 표현 방식.

**Semantic Map**
   - 슬램을 하면서 이 도시를 돌아다니면서 바닥에 있는 차선이라든지 로드 마커 같은 것들을 매핑을 해놓은 것
   - 포인트 클라우드나 이런 것들은 정보 하나하나를 하나하나씩은 작지만 다 모으고 나면 굉장히 무겁다.
   - 그런 무거운 것들 싹 빼고, 신호등의 위치 이런 것들만 저장을 해놓으면 훨씬 더 가볍게 쓸 수 있기 때문에 저런 것들을 이제 모아서 의미가 있는 것들만 모아놓는다.

### Simultaneous

- Simultaneous는 이제 위치 추정과 지도 작성을 동시에 한다는 것.
- 위치 추정을 한다는 거는 저희가 어떤 공간 안에 있을 때 지도를 보고 내 위치를 찾는 건데 지도가 있어야 된다.지도가 없으면 못 찾는다. 무인도 위에다 딱 떨어뜨려놓고 나서 눈을 딱 뜨면, 너 어디 있어? 그러면 글쎄... 이 얘기가 나올 수밖에 없다.
- 지도가 부정확할 수도 있다. 나를 에버랜드에 떨궈놓고 나서 롯데월드 지도를 보고 있으면, 보고 나서 뭐야 막 이렇게 된다. 분명한 것은, 지도가 부정확하면 자기 위치 찾는 것도 어려워진다.
- 그래서 정확한 지도를 필요로 한다.
- 그런데, 반대로 지도 작성을 한다는 것은 정확한 위치를 알아야 한다. 지금 보면 여기 뒤에 오피스에 이렇게 하나 좀 튀어나온 벽이 있고 저기도 튀어나온 벽이 있는데, 라이다를 가지고 여기서 쏘고 저기서 쐈으면 라이더 입장에서는 저 튀어나온 벽과 이 튀어나온 벽을 구분할 수가 없다. 둘 다 똑같이 튀어나온 벽이고 비슷하게 생겼기 때문에. 이거를 구분할 수 있는 방법은 어디 어디서 찍었는지 위치를 알려주는 방법밖에 없다.
- 그렇기 때문에 정확한 지도작성은 정확한 위치를 필요로 한다. 근데 이게 되게 어렵다.

### SLAM

- 위치 추정은 지도가 있어야 되고, 지도 작성은 위치를 알아야 되는데 슬램은 그런 거 다 필요 없다. 그냥 아무것도 모르는 상태에서 아무런 위치 정보 없이, 지도 정보 없이, 바로 위치 추정과 지도 작성을 동시에 푸는 방법.
- 조금 더 확률적으로 얘기하면 아무런 사전 정보 (prior 정보) 없이 시작할 수 있는 시스템이라고 얘기를 많이 한다.
- 아무것도 보고 있지 않은 상태에서 보이는 것들을 계속 이렇게 붙여나가는데, 그럼 붙여나갈 때는 어떻게 붙여나가느냐? 자기의 위치 정보를 새롭게 추론해 낸 정보를 가지고 계속 덧대어서 붙여나가는 형태로 볼 수 있다.
- 근데 그러면 이게 어떻게 되는 거냐. 그렇게 쉬운 거였으면 로컬라이제이션 문제도 아니면 매핑 문제도 그냥 다 진작에 해결됐을 텐데 이게 왜 어렵냐.
- 슬램에서 가장 중요한 부분은 이거다. 먼저, motion 모델이 있고 observation 모델이 있다.
- motion 모델은 로봇의 이동치를 표현하는 수식. 즉, 앞으로 한 걸음 갔다 그러면 한 걸음 같다라는 걸 표현을 해주는 수식
   - 근데, 실제로는 노이즈가 있다. 모든 로봇에서는 센서들이 달려 있고 센서들은 모두 노이즈를 가지고 있다. 1m 라고 생각했지만 실제로는 1m 1cm를 걸었다. 1m 1m인 줄 알았지만 99cm를 걸었다.
   - 계속 걷다 보면 이 오차가 점점 쌓인다.
- 반대로 observation 모델은 주변 환경에 대한 관측치를 얘기를 하는 것. 앞에 계신 분하고 저 사이에 거리 관계를 표현하는 방법.
   - 만약 모션 모델을 그대로 믿고 관측치들을 다 그냥 붙여보면, 그러니까 이동하면서 센서에서 나타난 값들을 가지고 그대로 이어붙여보면? 안 맞는다.
   - 안 맞을 수밖에 없는 게, 모션 모델만 그대로 믿고 갔었을 때는 모션 모델의 쌓인 오차 때문에 observation 모델도 말이 안 되게 된다.
- 그러면 이거를 어떻게 해야지 되냐?
- Motion 모델과 Observation 모델을 둘 다 엮었을 때, 오차의 합이 최소화 되는 부분이 어떤 것이냐라고 생각해볼 수 있다.
- 그때부터 motion 모델에 대한 부분도 조금씩 수정을 하고 observation 모델에 대한 부분도 조금씩 수정을 하는 것이다. 그래서 실제로 모션이 여기는 한 120도로 꺾었다 한 90도 꺾었다 이렇게 생각했지만 120도가 아니라 넌 여기서 90도 꺾었어 여기도 90도 여기도 90도 꺾었어 이런 식으로 바꿔주는 것.
- 그렇게 되면 이 로봇이 이 자동차가 어떤 위치를 이렇게 지나갔었음을 알 수가 있고 그리고 그 위치에서 어떤 랜드마크를 어떻게 바라봤는지에 대해서 정확하게 알 수가 있다.

### Factor graph, 그리고 Front-end / Back-end

- 슬램 쪽에서는 이런 문제를 이제 Factor graph라는 형태로 표현을 한다.
- 로봇이 움직이면서 이렇게 새로운 그래프의 노드와 엣지를 만들면서 갈 텐데, 그걸 저희가 graph construction 단계, 이걸 다른 말로 Front-end라고 부른다.
- 반대로 그래프가 다 만들어지고 나서, 방금 전처럼 이렇게 에러가 이렇게 많이 쌓여 있는 거를 최적화를 통해서 이렇게 풀어주는 작업은 graph-optimization, 다른 말로는 Back-end라고 얘기를 한다.
- 웹에서 얘기하는 프론트엔드 백엔드랑은 조금 다르다.
- 그래서 흔히, 슬램을 확률적인 그래프 최적화 문제다라고 이렇게 얘기를 한다.

---

## History of SLAM

### EKF SLAM (1990s)

**Backend: EKF**

**Complexity: O(n^2)**

- 우선은 가장 먼저 90년대에 de-facto, 그러니까 모두가 쓰는 방법으로 쓰였던 게 EKF SLAM.
- Extended Kalman Filter를 사용한다는 것.
- 90년대는 사실 EKF로 뭘 할 수 있을까 해서, 진짜 모든 보이는 거 다 EKF에 넣어보는 시대였다.
- 이제 많이 EKF 해보신 분들은 아시겠지만 안에 공분산 행렬, 즉 Covariance matrix 라는 걸 관리를 하게 된다.
- 여기서 이제 보이는 것처럼 로봇이 이동을 하면서 주변 랜드마크들을 볼 때마다 관측을 하게 되면서 EKF 안에 공분산 매트릭스를 관리를 하면서 지금 이렇게 uncertainty를 이렇게 표현을 하게 된다.
- 다시 보면 볼수록 uncertainty가 작아지기 때문에 점점 이렇게 타원이 작아지는 걸 볼 수가 있다.
- 그러면 슬램 EKF로 풀면 되겠다라고 했는데, 그렇게 많이 슬램이 90년대는 뜨지 않았다.
- 왜냐하면 EKF의 covariance matrix는, 관측치가 많아지면 많아질수록 매트릭스에 로우가 하나씩 많아진다.로우만 많아지는 게 아니라 똑같이 컬럼도 많아진다.
- 그러면 하나씩 랜드마크가 추가되면 추가될수록 O(n^2) 으로 올라가기 때문에 금방 메모리가 꽉 차버리게 된다.
- 이 방만 한 바퀴 돌아도 이미 끝난다. 컴퓨터 막 죽으려고 할 거다.

### Fast SLAM (2002)

**Backend: PF(Particle Filter) + EKF for each landmark**

**Complexity: O(n)**

- 다음으로 개선된 것이 Fast SLAM.
- Sebastian Thrun이 만들었다. 쓰런 아저씨는 지금의 유다시티를 만든 사람이기도 하고, 구글 웨이모를 창립한 사람이기도 하다. 웨이모 창립하고, 갑자기 나는 동영상 강의를 만들겠다고 나갔다. 내 롤 모델이다.
- Fast SLAM 같은 경우, O(n^2)이었던 친구가 O(n)으로 줄어들었다.
- 이것도 내용 깊게 들어가면 좀 굉장히 어렵기는 한데, 파티클 필터 방식으로 백엔드를 바꿨다.
- 랜드마크 하나하나씩을 트래킹하는 데에다가 ekf를 넣었는데, 그럼 이것도 ekf 쓰는데 이것도 O(n^2) 아니냐라고 할 수 있지만, 랜드마크 하나에 대한 것만 EKF를 하기 때문에 그렇게 observation 수가 많아지지가 않는다.
- 그래서 굉장히 효율적으로 이걸 돌릴 수 있게 됐다고 한다.

### 2D LiDAR SLAM (2010s)

**1. Gmapping (2005)**

**2. HectorSLAM (2011)**

**3. Cartographer (2015)**

- 2D 라이다 슬램은 여기까지 오면 거의 웬만한 현업에서 쓰고 있는 거랑 거의 똑같은 레벨로 올라왔다고 보면 된다.
- 2005년에 g-mapping이라는 알고리즘이 나왔고, 2011년에 헥터 슬램이라는 애가 나왔고. 사실 영상 보면 다 그게 그것처럼 보인다.
- 근데 뭐가 다르냐라고 하면 성능이 다르다. Cartographer가 제일 좋다. 구글에서 만든 거고, 2D 라이다도 쓸 수 있고 3D 라이다도 쓸 수 있다.
- 어떻게 둘 다 되냐라고 하면, 3D 라이다에서 나오는 모든 정보들을 2D로 압축을 해가지고 라이다 오도매트리를 한다.
- 당연히 Loop closure도 된다.
- 아까 EKF SLAM 같은 경우는 여기 한 바퀴 돌면 컴퓨터 터질 거라고 말씀을 드렸는데, 얘 같은 경우는 공장 한 바퀴를 다 돌더라도 거뜬히 견딜 수 있는 매우 효율적인 알고리즘이다.

### MonoSLAM (2003)

- 카메라 기반으로 슬램을 하는 첫 번째 시도가 나타난 분기점.
- 이거는 단순히 EKF SLAM을 카메라로 포팅했다 정도로 볼 수 있다.
- 근데 이제 비주얼 슬램을 처음으로 하면서 이런 고민이 생기게 된다.
   - 라이더 슬램 같은 경우는 처음부터 랜드마크의 포인트가 3D 공간 또는 2D 공간에 있는 걸로 나온다.
   - 근데 카메라 같은 경우는 3D 공간에 있는 정보가 2D의 이미지로 압축돼서 나오기 때문에, 거리 값 (depth)라고 하는 하나의 차원의 정보가 소실이 되게 된다.
   - 이거를 복원하기 위해서 어떤 방법을 거쳐야 되는가라는 부분은 영상 처리 및 3D geometry 관련으로 연구가 굉장히 많이 진행이 되었다.
- 그래서 모노 슬램이라는 것 그러니까 이제 카메라로도 이게 가능하다라는 걸 보였다.

- 기본적으로 EKF SLAM하고 굉장히 비슷하다. 때문에 얘도 공간을 그렇게 크게 못 쓴다. 여기는 주방 하나의 벽면 정도만 할 수 있다라고 하는데, 최근에 사용하는 슬램들하고 비교했었을 때, 사용하는 랜드마크 포인트가 현저히 적다. 얘는 30개 이상 포인트를 쓴다 그러면 컴퓨터가 녹아내린다.
- 근데 최근에 쓰이는 것들은 한 2만 개, 3만 개 잡아도 괜찮다.

### PTAM (2007)

- 2007년에 굉장히 큰 breakthrough가 나타난다. 개인적인 의견이지만, 슬램 쪽에서 있었던 가장 큰 breakthrough라고 생각된다.

- 근데 웃긴 것은, 연구자들이 만든 breakthrough가 아니다.

- 2006년쯤, 처음으로 인텔 듀얼 코어가 나왔다. CPU 코어가 2개가 된 것.

- 동시에 두 가지 일을 할 수 있게 된 건데, 슬램에서도 똑같이 했다.
   - 하나의 스레드에서 무거운 연산을 다 돌리기에는 너무나 오래 걸린다.
   - 그러면 빨리빨리 실시간으로 돌 수 있는 걸 하나 넣고
   - 조금 오래 걸리더라도 정확한 연산할 수 있는 거를 또 넣자
- 따라서 Tracking thread와 Mapping thread를 분리했다.
- 그러면 수많은 데이터들을 가지고 빨리 도는 연산들은 실시간으로 빨리 돌리고, 그다음에 좀 오래 걸려도 되는 것들, 한 200ms 500ms 걸려도 되는 것들은 뒤에서 돌린다는 방식으로 돌아가게 된다.
- 이 논문이 굉장히 대단한 게, 가끔씩 학부생들이 졸업 논문으로 쓰는 슬램 논문과 코드들이 있는데, 그것들과 정확도를 비교해보면 아직도 얘네 것들이 더 높은 경우가 많이 있다. 무려 2007년, 2008년 기술인데 굉장하다.
- 현업에서 앱을 만드는 회사들도 많이들 이 2007년 수준을 넘지 못해서 포기하는 곳들도 많다.

### Feature-based VSLAM (2010s)

**1. ORB SLAM (2015)**

**2. ORB SLAM 2 (2017)**

**3. VINS Mono (2017)**

**4. ORB SLAM 3 (2020)**

- 보통 이제 커뮤니티에서 비주얼 슬램 공부하고 싶은데 뭐부터 보면 될까요 하면 orb slam 보세요 한다.
- 읽지 마라. 그 첫 시작 논문으로 그거 읽지 마라. 코드도 보지 말아라. 그것부터 시작하면 너무 어렵고 별로여서 다 포기하게 된다. 정말 어렵다.
- 첫 논문으로는, PTAM을 추천한다.
- 이게 그래서 어느 정도가 됐냐라고 하면, 아까 모노슬램 같은 경우는 벽면 하나 그리고 PTAM 같은 경우는 조그마한 방 하나
- 이제 이거 같은 경우는 운동장을 이렇게 삥 돌면서 운동장에서 자기네 시스템 이름이 VINS MONO인데, 그래서 VINS라고 적고 막 뺑 돈다. 그리고 그거를 이제 항공지도에다가 덧대어서 그리는데 매우 정확하게 나온다. 실제로 굉장히 정확하다.
- 하지만 현업에서는 쓰기 어렵다. 다 라이센스 걸려 있고. 그리고 라이센스 피해 가신다고 해도 orb는 절대 쓰지 말아라. 성능 되게 안 좋다. 차라리 쓸 거면 VINS Mono

### Direct SLAM - DTAM (2010)

**PTAM의 쓰레드 분리를 이용**

**Direct 최적화 방식**

- Direct 슬램은 Feature-based랑은 굉장히 다르다. 일단 지금 나오는 것만해도 굉장히 다름. 아까는 굉장히 점들이 많이 찍혀 있었는데 점이 찍혀 있는 게 아니라 거의 막 벽 하나가 실제로 만들어지는 듯한 느낌.
- 이게 이게 어떤 점에서 다르냐라고 하면, 아까 봤었던 VINS Mono 같은 것들은 이미지에서 특징점들을 찾아낸 다음에 걔를 트래킹해서 걔가 딴 각도에서 봐도 제대로 된 위치에 있나를 보려고 하는 것.
- Direct는 그런 방법이 아니라, 그냥 이미지 전체를 다 보고, 이 이미지가 여기 있으면 내가 이거를 여기서 본다 그러면 색깔이 똑같나를 보는 것.
- Feature-baded SLAM에서는 포인트가 한 10개 있다. 근데 한 7개가 틀려버렸다. 그러면 3개만 옳은 거고. 근데 어떤 게 틀린지 모르는 상태. 그럼 이걸 가지고 어떻게 정확한 걸 알아낼 수 있을까? 못한다. 많이 어렵다. 엄청나게 많은 계산이 추가로 들어가야 되고, 그거 한다고 해도 잘 나올지 보장을 할 수 없다.
- 다이렉트 같은 경우는 이미지 하나가 여기 있고 이미지 하나가 여기 있는데 640x480 이미지만 해도 벌써 10만 개가 넘으니까 굉장히 정확하다.
- 그렇기 때문에 여기 지금 영상에서 나오는 것처럼 아까 막 엄청 막 흔드는 영상, 빠른 모션에도 흔들리지 않고 잘 붙어 있다라고 하는 것.
- 다이렉트는 굉장히 잘 붙어 있고 feature-based는 가끔씩 팍팍 튀는 모습이 있다.

### LSD SLAM (2014)

**최초의 large-scale SLAM**

**Direct 최적화 방식**

- ORB slam 저자들이 자기들이 알기로는 자기들이 제일 처음으로 complete full slam을 만들었고 large-scale을 커버할 수 있는 세상 첫 번째 시스템이다라고 했는데, 사실은 바로 1년 전에 이미 large-scale Direct slam 나왔다. 나왔는데 그냥 무시하고 올린 것...?

- 사실 large-scale을 먼저 한 쪽은 이쪽이다. 다이렉트 슬램 쪽이지만 건물을 한 바퀴 돌고 와서도 된다. 동네 한 바퀴를 돌고 와서도 된다라는 걸 잘 증명해낸 논문.
- 실제로 이거를 사용을 해가지고 나온 AR 어플리케이션도 굉장히 많았다.
- 당시에 2014년에 나왔었는데 2016년 17년에도 후속 연구들이 나왔고.
- 옴니아 핸드폰에서 처음에는 PTAM으로 포팅을 했었다가 PTAM에서 나타난 문제들이 몇 개 있으니까 LSD SLAM으로 한 번 중간에 업데이트를 한번 했었다는 것도 들었다.
- 지금 보시는 것처럼 이렇게 건물의 옥상을 한 바퀴 이렇게 돌고 와서 루프 클로저도 가능하고 이렇게 해서 스케일 드리프트도 없애고 이런 모습을 잘 보여준다.
- 맵도 훨씬 더 사람 눈에 좋게 보인다. Feature-based slam에서는 후추가루 이렇게 탁 턴 것처럼 점들이 몇 개가 동동 떠다니는 게 다였다면, 여기는 정말 엣지들을 다 보여주기 때문에 사람이 봐서도 여기가 여기 씬이구나라는 걸 어느 정도 알 수가 있다.

### Direct VSLAM  (mid 2010s)

**1. SVO (2015) - 400FPS :** 드론에 유용할 듯하다

**2. DSO (2017)**

**3. DM-VIO (2021)**

- 설명은 생략.

### KinectFusion (2012)

**RGB-D SLAM**

- Direct SLAM 가장 첫 번째 DTAM을 연구했던 분이 디탐을 연구하고 나서 느낀 부분: 아, 못 해먹겠다.

- 그래서 RGB-D 쪽으로 꺾었다.
- 딱 타이밍이 정말 절묘했다. DTAM을 연구하고 나서 느꼈었던 것이,  정말 이런 무거운 알고리즘까지 만들어서  슬램을 해야 되나라는 생각을 하고 있었는데, 그러던 참에 마이크로소프트에서 키넥트라는 게임을 런칭을 했다가 게임을 완전히 말아먹고, 그 센서들 재고로 남은 걸 어떻게 처리할까 하다가 컴퓨터 비전 연구하는 사람들이 우리가 쓸게요! 라고 해서 쓴 것. 
- 원래는 똑같은 성능의 센서가 그 당시에는 거의 막 한 몇만 달러 주고 사야 되는 거였는데, 근데 마이크로소프트는 그걸 적자 보고 팔 생각하고 싸게 팔았다. 거의 하나에 한 70달러 100달러 이러고 팔았다. 그때부터 RGB-D에 대한 연구가 엄청나게 올라왔다.
- KinectFusion으로 인해 3D 매핑에 대한 인식 자체가 바뀌었다고 한다.
   - 2012년이면 orb 슬램도 나오기도 전. 이 때까지 사람들이 봤던 건 PTAM / DTAM 밖에 없었다. PTAM은 점 몇 개만 이렇게 해서 동동 떠다니는 거고, DTAM은 멋있기는 한데 엄청나게 무거운 GPU랑 CPU 다 끌어다 써갖고 겨우 되는 거였으니까.

   - 이제 어떻게 해야 되지? 하고 있었는데 가벼운 CPU와 GPU의 노트북에서도 돌아간다라는 Kinect Fusion이 나오니까 이제 완전히 새로운 시대가 열렸다라고 한 것.
   - 물론 노트북에서 돌아간다고 하지만 실제 데모를 들고 간 거 보면 거의 한 5kg짜리 노트북을 들고 온다. 어쨋든 노트북에서도 된다.

### RGB-D SLAM (2010s)

**1. RTAB-Map (2014)**

**2. ElasticFusion (2015)**

**3. BundleFusion (2017)**

- KinectFusion이 계속 발전을 해서 결국에는 훨씬 더 정밀하게 3D 공간을 매핑할 수 있는, 그것도 훨씬 더 가볍게 매핑할 수 있는 방법이 뭘까 연구가 많이 되었다.
- 사실상 RGB-D 기반의 3D Reconstruction 쪽으로는 여기서 끝났다고 봐도 무방함.

---

### LOAM (2014)

- 2014년에 나타난 또 하나의 breakthrough 논문
- 3D 라이다를 가지고 살짝 PTAM과 비슷하게 두 개의 쓰레드를 나눠서 어떻게 할 수 있는가, 그리고 포인트 클라우드를 3d에서 매핑을 하려면 어떻게 해야 되는가에 대한 부분을 굉장히 잘 한 논문
- 이 당시에는 굉장히 느렸다. 되기는 되는데 엄청 느리다라는 평이 많았다.
- 근데 잘 된다는 걸 아니까, 그때부터 지금까지 계속 나오는 연구들이 어떻게 하면 이거 빨리 돌릴 수 있을까에 대한 연구가 주로 이루어졌다고 볼 수 있다.

### 3D LiDAR SLAM (2019 ~)

**1. ALOAM (2019)**

**2. HDL-SLAM (2019)**

**3. FAST-LIO (2021)**

- 이제는 엄청 빠르다. 더 빨라졌다, 정확하다. 그런 것. HDL SLAM은 약간 다른 성격이긴 하지만 어쨌든 그렇다.


---

## Deep Learning + SLAM

슬램은 그동안 딥러닝이랑 좀 거리가 먼 기술이었다.
   - 왜냐하면 일단 딥러닝은 파이썬을 많이 쓰는데, 반면에 슬램은 많이들 c++을 쓰고. SLAM도 그럼 파이썬으로 넘어오면 되지 않느냐라고 하는데, 슬램을 파이썬에서 돌리기는 굉장히 어렵다. 파이썬 언어가 가지는 한계가 있다.
   - 파이썬에서는 독립된 작업을 독립된 스레드에서 돌리는 것은 굉장히 쉽고, 똑같은 작업들을 다른 스레드에서 돌리는 건 쉬운데, 다른 작업에서 서로 다른 스레드 간 소통을 하면서 돌리는 것은 굉장히 어렵다. 파이썬 언어가 그거를 못하게 만들었다(GIL).
   - 근데 슬램은 그게 필요하다. 그게 코어가 되는 애들이 있다. 그래서 굉장히 어렵다. 그래서 어쩔 수 없이 스레드 하나하나를 직접 다루게 되는 C++을 써야 한다.

하지만, 딥러닝은 슬램에 다가올 수밖에 없는 미래라고 본다. 
- 앞으로 슬램은 딥러닝을 반드시 수용을 해야 되고, 딥러닝도 슬램을 필요로 하게 될 것이다. 
- 왜냐하면 슬램이 줄 수 있는 이 굉장히 조그마한 하나의 점이 있는데 이 하나의 장점으로 인해서 앞으로 3D 비전을 할 때 정확도가 좋아지냐 나빠지느냐는 천지 차이가 나게 될 것이다. 이것 때문에 딥러닝을 하는 사람들도 슬램을 이해하고 받아들일 준비가 되어야 한다.

Q. 딥러닝을 왜 쓸까?
   - 딥러닝을 보통 사람들이 이렇게 얘기를 한다.
   - 엄청나게 많은 데이터의 양으로부터 사람이 직접 뭔가 만들기 어려운 수준의 룰과 패턴을 컴퓨터가 학습하게 한다.
   - 예를 들어서 낮과 밤에 어떻게 다른가 어떤 물체가 다 낮에는 어떻게 보이고 밤에는 어떻게 보이고 뭐 이런 것들이 있을 건데 그걸 사람이 직접 하나하나씩 코딩을 하기엔 너무나도 많고 비효율적이다.
   - 딥러닝으로 데이터를 우리가 모아주고 라벨링을 해주면 컴퓨터가 알아서 가장 좋은 representation을 찾아낼 것이다라는 거를 믿고 가는 것.
   - 슬램에서는 가장 중요하게 보는 세 가지가 있다.
      1. 정확도 (Accuracy)
      2. 속도 (Efficiency)
      3. 강인함 (Robustness)
   - 강인함은 쉽게말해 어디서든 작동을 할 수 있어야 된다는 건데. 슬램에서 지금까지 웬만한 경우는 사람들이 다 손으로 만들었다. 그래서 하나의 씬에서만 작동하는 경우가 많다.
   - 여기서는 작동했는데 바로 옆에 저 방으로 갖고 갔을 때 잘 안 되는 경우가 너무 많다.
   - 딥러닝이 바로 이 부분을 해결해 줄 수 있다. 그런 특정한 부분을 딥러닝으로 데이터를 수집해서 가장 적당한 generalization이 잘 되는 representation으로 바꿔주면 된다.

### SuperPoint + SuperGlue

**: CVPR 2020 SLAM Challenge Winner**

- 이미지 속에서 코너점들을 찾아내고 매칭을 잘해주는 네트워크를 사용하여 1등함.
- 슈퍼포인트는 점들을 찍어주는 역할
- 슈퍼글루는 이 이어지는 이어지는 프레임들 사이에서 이거를 연결해주는 역할
- 슬램에서 이 정도까지 점 잘 붙어서 나오는 애 있다고 하면 그거는 슬램 5년 차 이상의 엔지니어가 튜닝을 굉장히 잘하는 경우이다. 근데 그게 하나의 신에서만 작동할 확률이 굉장히 높다. 이 씬을 가지고 밤에 있는 씬으로 갔을 때는 안 될 확률이 99%다.
- 딥러닝은 밤에 있는 데이터까지 넣어주면 될 거다. 아니면 적어도 될 거라는 희망이 있다.
- '그래도 사람이 직접 짜는 게 좋다' 이렇게 얘기하는 사람도 있는데, 결국 2020년도 슬램 챌린지 우승자는 orb slam framework 위에다 그대로 슈퍼포인트 슈퍼글루 이것만 바꿔준 것이었다.
- 정말 그냥 모듈만 바꿔 낀 것이다. 별거 한 게 없다. 심지어 fine-tuning도 안했다.
   - 우승자들이 얘기를 한 게, 우리는 그냥 막 인터넷에 있는 거 그냥 갖다 썼다. 왜냐하면 우리는 이 데이터셋에 overfitting을 안 했다고 보여주고 싶어서라고.
   - 근데 내 생각에는 그냥 귀찮아서 안 한 거다. 근데 그래도 1등이었다.

### NetVLAD

**: CVPR 2020 Place Recognition Challenge Winner**

- 넷플라드란, 여기 공간과 여기 공간은 실제로 같은 공간입니다라는 것을 인식하는 기술 중 하나.
- 여기 보시면 여기 통이 이 통과 같은 건데 조금 다른 각도에서 찍었고. 그리고 낮밤이 다르다.
- 넷플라드 같은 경우는 이 두 신이 같은 신을 보고 있다고 알 수 있다. 즉, 이 두 이미지가 같은 씬을 보고 있다는 걸 인지를 할 수 있는 기능이다.
- 슬램에서는 어디에서 많이 쓰게 되냐면, 루프 클로저를 할 때 많이 쓰게 된다.
- 예전에는 SLAM이 이걸 못 했다. 낮밤을 구분을 못했다. 똑같이 생기지 않으면 구분을 못했다. 근데 이제 넷플라드같이 딥러닝 방식으로, data-driven 방식으로 하면 이거를 할 수가 있다.
- 실제로 그렇게 해서 CVPR 2020년도에 Place Recognition 대회에서 넷플라드 (정확히는 넷플라드를 개선시킨 패티 넷블라드였나)가 1등을 했다.

### Depth Estimation / Completion

- 또 딥러닝 많이 쓰인 것들 중 하나는 Depth estimation

- 예전에는 이미지 하나를 가지고 뎁스를 알 수는 없었지만 딥러닝을 가지고 뎁스를 알 수도 있게 되었다.
- 또한 RGB-D 센서나 라이다 센서 같은 걸 썼을 때, 거리 센서의 맥시멈 거리를 넘어가 버리게 되면 정확한 depth 가 아닌 쓰레기 값이 들어오게 된다. 그래서 활용할 때 이거를 계산해서 제외해 갖고 해야 되는데 조금 안 좋다. (실제로는 최대측정값보다 조금 더 멀리 있는 걸 수도 있는데)
- 그럼 어떻게 할 수 있냐. 이거를 딥러닝으로 봐서 이거를 실제 raw 데이터랑 섞을 수 있다.그렇게 해서 비어있는 depth 값을 채워주는 depth completion. 이런 것들을 써서 좀 더 안정적인 depth를 뽑아서 슬램에다가 넣어줄 수 있다.

### Object Detection / Semantic Segmentation

- Mask Dynamic objects
- 슬램에서 지도를 작성하고 있는 도중에 누가 움직이거나 지도가, 공간이 움직여버리면 이 계산이 안 맞는다. 그래서 슬램이 터지는 원인이 된다. 그래서 슬램을 할 때 최대한 움직이는 게 없어야 된다라고 이렇게 얘기를 한다. 모두 그냥 그대로 멈춰라 하고 다 멈춰야 되는 거다. 근데 그럴 수는 없는 거다.
- 그러면, 딥러닝을 써서 움직이는 애들을 세그멘테이션을 걸러가지고 없는 사람 취급해버리자 투명 인간 만들어버리자라고 하는 거다. 이런 기술들도 있다.

### 기존의 SLAM pipeline과 DL 모듈

- 이것만 넣어도 웬만한 지금 나와 있는 논문들은 다 다 씹어버릴 수 있다.
   - 이미지가 들어왔을 때 원래는 orb를 뽑는데, orb가 아니라 슈퍼 포인트를 먼저 뽑자
   - 그리고 오브젝트 디텍션과 세그멘테이션 정보를 같이 넣자.
   - 그리고 local map 트래킹 할 때, 예전에는 디스크립터 기반으로 엮었는데 그게 아니라 슈퍼 포인트와 잘 맞는 슈퍼 글루를 넣자.
   - Depth estimation 정보를 통해서 3D 결과를 함께 사용하자.
   - 나중에 루프 클로저를 할 때는 넷플라드를 사용하자.
- 성능이 나쁠 수가 없다. 근데 그러면 잘 된 거 아닌가? 슬램은 이걸로 끝났네라고 할 수 있다. 실제로 그렇다.
- 그렇지만 나는 그렇게 생각 안 한다. 먼저 딥러닝 방식에는 가장 큰 문제가 하나 있다
- 우리가 이미지 스페이스에서 잘 된다는 걸 알고 있다. Object Detection, Semantic Segmentation, Depth Estimation, ... 기가 막히게 잘 된다.
- **3D 공간 안에서도 이게 잘 될까? 이게 중요하다.**
- 3D Object Detection, 3D semantic segmentation. 잘 될지 안 될지. 과연 뉴럴 네트워크가 3D 공간의 정보를 이해할 수 있을까라는 게 핵심 질문.
- 물론 논문들 보면 다 잘 된다고 한다. 자기들 데이터셋 안에서는.
- 근데 이제 이미지넷 같은 걸 통해서 거기에서 검증이 된 애들이 실제 세상에서 잘 될지 안 되는지 검증이 되듯이, 3D 오브젝트 디텍션, 3D 세그틱 세그멘테이션도 그게 검증이 되어야만 한다.
- 아직까지는 그게 안 되고 있다. 아직까지는 3D 오브젝트 디텍션이나 이런 게 정말 제너럴하게 잘 되는 경우가 없다.

### DL 모듈 활용의 문제점

1. **충분한 3D 데이터를 준비할 수 있을까?**
2. **Geometry에 대한 모델은 이미 저명함** → 굳이 이것을 Learning 방식으로 대체할 이유?
- 어떻게 잘 되게 할까? 가장 심플한 것은 딥러닝에서 문제가 안 풀릴 때는 어떻게 한다? 데이터가 부족하다고 얘기를 한다. 양질의 3D 데이터를 준비를 하면 된다.
- 근데 3D 데이터를 준비를 할 수 있을까가 문제다.
   - 예를 들어서 63빌딩에 대한 데이터를 모아본다고 해볼게요.
   - 이거에 대한 3d 데이터를 준비를 하려고 하면 63빌딩을 볼 수 있는 가능한 모든 각도에서 사진을 다 찍어야 돼요. 가능할까요?
   - 밑에서 보이는 것들은 찍을 수 있겠지만 중간에 공중에 뜬 걸 어떻게 할 것이고 하늘에서 본 건 어떻게 할 것이고 그리고 낮에도 그걸 찍어야 되고 밤에도 찍어야 되고 광원이 바뀔 때도 찍어야 되고 뭐가 지나갈 때도 찍어야 되고 이미지는 구할 수 있겠죠. 어떻게든.
   - 근데 그 이미지에 정확한 3d 위치까지 저희가 라벨링을 같이 넣어줘야 되는데 그게 가능할까요?
   - 이게 정말 mm 단위로 정확해야 되겠죠. 그렇지 않으면 노이즈한 데이터가 들어가니까.
   - 요즘에 앤드류 응 교수님도 그렇게 얘기를 하시죠. 이제 빅데이터의 시대가 아니라 굿데이터의 시대다. 노이즈가 없는 게 더 좋다.
   - 하지만 노이즈가 낄 수밖에 없어요. 많이들 데이터셋에서 그런 걸 많이 사용해요.
   - GPS 정보를 pose ground truth로 사용한다? GPS의 오차는 미국에서도 +- 10m입니다. 우리나라는 +- 50m예요. 왜냐하면 우리나라에 GPS가 없기 때문에. KPS 만든다고 해요. KPS 그거 2035년에 끝날 예정입니다. 그러면 우리나라에서는 일단 못 만들죠.
- 또한, 지오메트리에 대한 모델은 사실 이미 다 수학적으로 풀렸다. 따라서 우리가 이미 모델이 있는데 왜 굳이 러닝 방식을 왜 다시 학습하려고 하냐라는 의문을 피할 수 없다.
   - 지오메트리에 있는 모델 쓰면 안 되냐. 빛이 물체에 반사돼서 렌즈 안에 들어와서 투과하고 꺾여 들어와가지고 focal length principle axis 맞춰서 들어와서 한 점에 모여서 이미지가 만들어진다.
   - 이거는 다 풀린 건데 이걸 왜 러닝 방식으로 다시 바꿀까 이 얘기를 많이 하는 거죠.

### Deep Learning 관련 다양한 시도들

- 3D geometry를 한 번에 다 학습하는 거는 처음에 누군가 해보려고 했는데, 논문거리가 안 나왔다.
- 그 당시에 데이터셋이 부족했을 수도 있고, 아이디어가 안 나왔을 수도 있고. 아니면 PC 자원이 부족했을 수도 있고.
- 그래서 geometry를 추정할 수 있는 prior를 학습해보자는 방향이 제시됨.
   - 뎁스 정보를 학습한다든지
   - 포즈 정보를 학습한다든지
   - 이미지 pair 정보를 학습한다든지

**UnDeepVO (2017)**

- 학습 용도로 스테레오 이미지를 쓴다.
- 추론 시에는 모노 이미지를 쓴다.
- 왼쪽 오른쪽 이 두 개를 가지고 ground truth 뎁스 이미지와, 두 이미지 사이의 포즈, 그리고 이를 기반으로 두 이미지 사이의 포즈를 추정하는 네트워크를 학습시킨다.
- 한쪽만 오른쪽 이미지만 넣으면, 얘가 하나의 이미지만 가지고 반대쪽에 있는 이미지가 어떻게 생겼을지 예측을 해서 뎁스 이미지를 만들고 포즈 정보를 만들어가지고 나오는 것.
- 뎁스 이미지가 나오기 때문에, 카메라에서 언프로젝션 하면 바로 3D 모양이 나오니까 맵도 만들 수 있다라고 함.
- 키티 데이터셋에다가 엄청 과적합 한 건데, 과적합 하니까 그래도 괜찮았다. 적당히 봐줄 만한 게 나왔다.

**TartanVO (2020)**

- End-to-end Visual odometry 같은 거라고 이해함.
- Matching network / Optical flow network / Pose network 전부 이어 붙임.
- 물론 완전 엔드투엔드는 아니지만, 첫 이미지들을 들어오는 걸 보고 나서 이미지 두 개를 매칭을 하고 옵티컬 플로우하고 포즈를 뽑는 네트워크를 셋 다 이어서 만든 것.
- 엔드투엔드라고 얘기한 거는 싱글 네트워크가 인풋 아웃풋을 한다는 게 아니라, 그 모든 게 네트워크로 이어져 있다라는 의미
- 얘가 그래서 장점이 뭐가 있냐라고 하면, orb 슬램 같은 경우는 갑자기 팍 움직이면 끊긴다. 피처가 트래킹이 안 돼서 끊기는 경우가 많다
- 하지만 딥러닝은 그걸 끊기지 않고 간다는 것.
- 아직 정확도 측면에서는 orb slam이 더 정확하다. 근데 그러면 orb 슬램이 더 좋은 거 아니냐라고 물어볼 수 있다.
- 정확도는 orb 슬램이 더 높다. 근데 orb는 중간에 끊긴다.
- 딥러닝은 안 끊긴다. 그래서 더 robustness가 높다라고 얘기를 한다.
- 딥러닝 방식을 썼더니 정확도는 잘 모르겠지만 안 끊긴다 이거는 이노베이션이다 이렇게 얘기를 하는 것.
- 근데 사실 슬램하는 사람들 입장에서 봤을 때는 끊기면 그냥 껐다 켜면 되지 이렇게 생각하는 경우도 많다.

**D3VO (2020)**

- Depth network / Pose network / Aleatoric uncertainty network
- 프런트엔드를 전부 딥러닝을 쓰고, 백앤드는 기존의 optimization framework을 사용

### 왜 SLAM에는 딥러닝을 적용해도 혁신이 생기지 않는가?

- 결국은 이런 여러 가지 방식을 했는데 결과는 뭐냐라고 하면 조금 더 개선된다는 것.
- 결국에는 아까 봤었던 것처럼 기존의 모듈을 딥러닝으로 갈아낀 것과 별다른 게 없다.
- 조금 더 다양한 환경에서 작동하고, 조금 덜 끊기고, 조금 더 정확도가 높아질 때도 있고.
- 근데 속도가 빨라지지는 않았다. 이건데 사실 연구니까 어쩔 수 없다. 사실 굳이 막 논문을 쓰는 사람들이 이거를 가속시켜서 엄청 빠르게 할 생각은 없고.
- 슬램에 정말 진심인 사람들 학교에 가면 다 얘기하고 있다. 딴 데는 딥러닝 적용하면 어쩐다. 이제 이 업계는 뒤집어졌어. 이제 막 이렇게 얘기를 하는데 왜 슬램은 왜 딥러닝 적용해도 그게 그거 같냐. 왜 안 좋아져. 괜히 슬랩 연구 끝났다라고 하는 게 아니다. 괜히 펀딩 안 들어오는 게 아니다 이렇게 얘기를 한다.
- 그래서 사람들이 많이 고민한다. 슬램의 본질이 뭐였지? 결국에는 우리가 풀고 싶어 하는 게 뭐지? 우리가 너무 그냥 딥러닝이 잘 된다고 생각하니까 딥러닝을 그냥 보이는 데다 대충 대충 붙여서 적용하느라 그런 게 된 게 아닐까... 정말 슬램과 딥러닝이 잘 만날 수 있는 곳이 어딜까를 굉장히 고민을 많이 한다.
- 결국 내가 있는 곳 주변은 어떤 공간이고 그리고 나는 그 안에 어디 있는가 이걸 풀려고 하는 것이다. 모듈이 어떻게 생겼고 피처 다이렉트 뭐가 됐든 상관이 없다. 지금까지 봤었던 라이다 카메라 다 필요 없다. 다 상관없다. 다 잊어버리고 새로 시작하자.
- 그리고 지금까지 만들었던 것의 단점들을 봤었을 때, 포인트 클라우드 맵, 메쉬 맵, occupancy grid map. 열심히 수학적으로 잘 된 건 맞긴 한데.
- **문제는 하나도 인터랙션이 가능한 게 없다.**
- 딥러닝으로 하면 뭔가 이게 강아지다 이게 고양이다. 이게 뭐 자동차다 뭐다 클래스 정보가 나와서 바로 액션을 어떻게 취할 수 있다 이런 게 나오는데 슬램은 그게 아니라 포인트 클라우드맵. 그래서 이걸 뭐 어떻게 하라고 벽이다. 아니다. 저기 안 처박기만 하면 되는 거야. 이거밖에 없는 상태.
- 그래서 내 주변에 있는 공간이 뭔데 포인트 클라우드 뭐라는 건지 별 의미가 없는 것. 내 주변은 어떤 공간이고라고 하려면 적어도 이건 테이블이야. 이건 노트북이야. 내 앞에 테이블이 있고 노트북이 있어. 노트북과 인터랙션 하려면 어떻게 어떻게 해야 돼 이런 게 있어야 되는데, 그런 거 전혀 없이 그냥 포인트 클라우드야 이러니까 답이 없다.
- 이때부터 spatial AI에 대한 단계를 나누게 되었다. 특히, 레벨 3랑 4가 새롭게 나타남.

## Spatial AI

**공간 인식의 단계**

1. **Pose + Sparse map**
   - ORB-SLAM
   - For localization purposes
1. **Dense map**
   - Elastic Fusion
   - 장애물 충돌회피 및 경로 생성
3. **Semantic understanding**
4. **Object-level, Dynamic map**

### Semantic understanding (Level 3)

- 가장 먼저, 2d Object Detection과 Semantic segmentation을 이미 잘 돼 있는 것들을 써보자.
- 기존에 슬램도 잘 되어 있고 오브젝트 디텍션 세마틱 세그멘테이션 잘 되어 있는데 기존에 나오는 맵 위에다가 세마틱 레이블을 얹어보자. 그러면 적어도 이 포인트 클라우드가 뭔지는 알 수 있겠다. 이게 의자다. 이게 노트북이다. 이거는 알 수 있겠다.
- 그럼 인터랙션을 할 수 있겠다라고 생각한 것.

**CNN SLAM (2017)**

- 기존에 svo에다가 Mask-RCNN을 그냥 얹은 거였나 그랬었던 걸로 기억.

**SemanticFusion (2017)**

- 얘는 ElasticFusion에다가 똑같은 걸 얹은 것.
- 둘 다 같은 해에 나왔는데 사람들이 생각하는 게 참 비슷하다.

**CubeSLAM (2017)**

- 이제 object detection으로 해볼까?라는 연구도 나왔다. 바운딩 박스를 이용하자는 것.
- 바운딩 박스가 결국 어떤 물체가 여기쯤 있다는 걸 얘기를 하는 건데, 여기서 봐도 바운딩 박스 여기서 찍힐 거고 여기서 봐도 바운딩 박스 여기서 찍힐 거니까 이 정보를 가지고 물체가 있다는 걸 표현을 해보자
- 바운딩 박스가 보이는 걸 가지고 물체를 이렇게 찍어주기 시작. 기존에 있었던 그냥 점들이 공중에 둥둥 떠다니는 슬랩하고는 많이 다른 형태.
- 실제로 내가 지금 운전을 하면서 자동차들이 어디 있고를 알 수 있는 상황인 거고 그러면 저 자동차들이 멈춰 있나 움직이고 있나 이것까지 구분할 수 있으면 자율주행에 대한 초석이 깔리게 되는 것.

**Fusion++(2018)**

- 슬램을 하면서 나타나는 3d 공간에서 그 위에다가 2d 이미지 mask-rcnn과 3D에서 나온 detection 정보를 조합을 해가지고 dense point cloud에 레이블을 얹어주는 방법.
- 레이블을 모든 것에다가 얹어주는 게 아니라, 내가 알고 있는 오브젝트들에만 할 수 있다.
- 한 번 이게 되고 나면 이 물체들을 움직이고 갖고 인터랙션을 할 수가 있다. 왜냐하면 물체들 사이의 관계를 계속 유지를 하려고 하기 때문에.
- 즉, object level의 semantic 맵이 나타나게 된다.

### Multi-modal fusion

- 2D image 로부터 얻을 수 있는 semantic 정보 (i.e. object detection, semantic segmentation)
- 3D 공간으로부터 얻을 수 있는 semantic 정보 (i.e. 3D object detection, 3D semantic segmentation)

### Deep Factor Optimization

- 동일 클래스에도 다양한 형태의 생김새가 있음.
   - Shape
   - Appearance
- 딥러닝 네트워크를 돌렸을 때, 이 네트워크 안에서는 추론할 수 있는 다양한 오브젝트 클래스들이 있을 것. 근데, 모든 오브젝트들이 똑같이 생긴 게 아니고 다른 형태를 가질 수도 있고, 다른 appearance를 가질 수도 있고.
- 의자지만 얘보다 조금 더 넓고 짧은 것도 의자고, 조금 더 가늘고 긴 것도 의자고. 즉, 클래스마다 어떤 distribution을 가지게 된다.
- 이 distribution을 가지고 우리가 지금 보고 있는 정보, 즉 observation 정보와 함께 조합을 하면 partially observable한 객체의 쉐입에 대한 정보를 구하는 것이 가능하다.
- 이게 그럼 장점이 뭐냐 하면, 옛날에는 shape 복원을 위해, 모든 형태의 데이터들을 다 한 번씩 본 적이 있어야지 네트워크가 잘 동작했다.
- 하지만 만약, 어떤 클래스에 대한 distribution만 알고 있다면 그 분포 안에서 실제로 본 적 없는 오브젝트의 형태도 정확하게 추론할 수 있다. 즉, zero-shot detection이 된다고 이해할 수 있다.
- 단순히 클래스만 구분하는 게 아니고, 클래스와 shape과 내가 그 공간 안에 있는 pose까지 구해낼 수 있다고 볼 수 있다.

**Example 1: Tesla Auto-labeling**

- 그래서 이거를 실제로 쓰고 있는 곳이 어디냐? 가장 앞서 있는 곳은 테슬라.
- 테슬라의 Auto-labeling 기술은, 정확한 3D perception 정보를 기반으로 2D 이미지 perception 성능을 높인다.
- 근데 이거를 실제로 운행하고 있는 차에서는 할 수가 없고, 서버에서 돌리는 것. 서버에서 돌려서 정확한 3d 공간을 만들면, 3D를 2D로 투영을 시켜서 이 정보를 가지고 2D 디텍션을 그리고 2D 세그멘테이션의 성능을 높인다.
- 옛날 같았으면 이게 라벨이 좋은 라벨인지 아닌지 모르겠다였는데, 그게 아니라 그냥 이 시퀀스를 가지고 3D 씬을 만들어버린 다음에, 정확한 슬램과 딥러닝을 섞어서 정확한 3D 씬을 만들고 그걸로 바로 2D 이미지 네트워크의 트레이닝에 바로 넣는 것.

**Example 2: Compressed deep factor**

- RGB-D SLAM이 다 여기로 와버렸다.
- RGBD SLAM은 더 이상 reconstruction을 하려고 하지 않고, RGB D 정보를 가지고 좋은 ground-truth 정보가 나오니까 이걸 기반으로 학습을 하고 좋은 딥러닝 슬램을 만들겠다라고 하는 중.

**CodeMapping (2021)**

- 모노 카메라임에도 불구하고 매우 dense한 씬을 만들면서 갈 수 있다는 걸 보여줌.
- 이미지가 들어올 때, 댑스와 포즈에 대한 정보를  auto-encoder로 압축을 시켜놓은 것.
- 오토인코더를 압축을 시켜놓고 나면, 굉장히 작은 벡터가 됨
- 한 30개 40개 정도의 정보가 있는 벡터가 되는데, 여기 하나하나에 있는 값을 슬램이 마음대로 바꿀 수 있음. 이걸 바꿈에 따라서 정확한 댑스로도 갈 수 있고 안 정확한 댑스로도 갈 수 있다. 정확한 포즈로도 갈 수 있고 안 정확한 포즈로도 갈 수 있다.
- 즉, 슬램이 이거를 정확한 방향으로 가게 만들어준다라고 생각을 하고 하는 것.

**NodeSLAM (2020)**

- 노드 슬램을 돌리기 전에 있는 네트워크는 컵은 컵이다. 컵이 어떤 형태로 만들어지는지 대충 알고 있다.
- slam 중 마주치는 컵은 각각 다 생김새가 다를 수 있다. 색깔도 다르고 두께도 다르고 굵기도 다르고 높이도 다르고.
- 네트워크는 이 다양한 컵들을 전부 본 적이 없다.
- 하지만, 슬램으로 돌아가면서 다양한 각도에서 봤을 때 점점 정확한 쉐입으로 최적화되는 걸 보여주겠다는 것.
- 즉, slam 최적화 과정에서 역으로 shape에 대한 정확한 추론이 가능해짐.
- 하나의 클래스 안에 있는 다양한 매니폴드 아내를 탐색하는 거를 보여주는 아주 좋은 예시

**NeRF**

- 2020년에 나오고 나서 굉장히 핫하다
- 왜 핫하냐고 하면, 3D 씬에 대한 정보 자체를 네트워크 하나에다 담을 수 있고, 그것을 이제 학습을 시킬 수 있다는 것.
- 학습을 시킬 수 있다는 것은 미분이 가능하다는 형태라는 것.
- 미분이 가능하다는 건 슬램에서도 옵티마이제이션을 할 수 있다는 얘기가 된다.

**iMAP (2021)**

- 따라서 슬램 안에다가 NeRF 네트워크를 아예 넣어버린 논문.

**NICE-SLAM (2022)**

- 마찬가지로 슬램 안에다가 NeRF 네트워크를 아예 넣어버린 논문.
- iMAP이 비효율적인 부분이 있어서 이를 개선했다.
   - 아이맵 같은 경우는 NeRF 네트워크는 작게 만들었지만, 공간 전체를 다 업데이트하려다 보니까 너무 느리다고 나왔었고, 나이트 슬램 같은 경우는 그런 네트워크들을 굉장히 많이 쪼개서 로컬 bundle adjustment 할 때 작은 단위로 하듯이 그렇게 해서 만들었다고 들었다.
- 이거를 쓰면 어떤 좋은 점이 있냐. NeRF를 슬램에다 넣었더니 어떤 효과가 있었냐

   →  안 보이는 곳에 대한 맵을 만들 수 있다는 것.

- 내가 이 신에 대해서 학습을 했고, 내가 본 적이 없는 곳에 대한 뷰를 만들 수 있기 때문에, 보이지 않는 곳에 대한, 그러니까 그림자가 진 곳에 대한 맵도 만들 수 있게 된다.
- 그렇게 되면 지금까지 없었던 정말 빽빽하게 나오는 맵을 만들 수가 있게 된다. 빈 곳이 하나도 없는 맵을 만들 수 있다.
- 그래서 나이스 슬램에서는 지금 보여주는 것처럼 구멍이 뚫린 곳이 한 곳도 없다.
- 그래서 NeRF를 써보는 방식들도 많이 나오고 있는데, 그래서 결국에 뭐가 제일 좋을 것 같냐라고 하면 솔직히 아직은 잘 모르겠다.
- 너프가 답이 될 거라고는 생각은 하지 않는다. 그렇게 단순하게 너프를 끼워넣었더니 세상에 모든 게 다 풀렸다라고는 생각하지 않는다.
- 하지만 여전히 너프는 굉장히 좋은 방법이라고 생각을 하고, 아직은 조금 더 다양한 시도가 있어야 될 것 같다.
- 정확한 3d 공간을 도출하기 위해서는 learning-based approach와 슬램의 probabilistic fusion이 둘 다 적절하게 섞여야 된다라는 것만큼은 확실해보인다.
- 너프를 쓰던 옵티마이즈 디팩터를 쓰던 어떤 방식을 쓰던, 결국에 공간에 대한 정확한 인식을 하기 위해서는 학습 기반 방법들은 제너럴라이제이션만 잘 되고 정확한 로컬라이제이션이 안 될 거기 때문에 그걸 정확하게 봐줄 수 있는 슬램이 무조건 붙어야 된다 라고 생각을 한다.

## 로드맵: 강의를 마치며...

![Image.png](https://res.craft.do/user/full/cbf4739d-a8fe-1bea-b815-c6e8343d6213/doc/C6E1AB32-944E-44BD-BA45-CD54096E47D9/B72B07A4-E7C3-4C18-AED3-48FAA489604C_2/8eKxUIL1bhTJicGnZ0yGIwhrrdPjA1bi3zBuh7HlUokz/Image.png)

- 이미지가 들어오면 많이들 사용하는 2D 딥러닝 방식을 사용. (2D)
   - Object detection / Object segmentation → Accurate 2D representation
- 이미지가 들어오면 슬램도 한다. (3D)
   - Deep factor optimization 이나 nerf, code등 기반의 최적화 → Accurate 3D representation
- 2D + 3D → Multi-modal fusion
- 이 중에서 얘가 없어도 안 되고 얘가 없어도 안 된다.